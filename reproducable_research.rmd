# Reproducable Research

As a scientist, the integrity of your data is your #1 priority. In fact, I argue that your ability to write advertisements of your research, which is what manuscripts really are, is secondary to the data that you collect and provide the community. As a practitioner, you get more credit and are providing more to your research community if your data are not only collected with integrity but able to be used by others. In the near future, the DOI for your data sets will be as important as the ones for your manuscripts.

For your data to have the most impact, it needs to be both available and valid. One way to help keep your data intact is to put it in one place and only pull from there to do analyses when you need to do them. You should ALWAYS keep your data and your analyses as close to each other as possible. Once they get separated, things go crazy. For example, consider the case where you send your colleague a copy of your data in a spreadsheet. Now you have two potentially different (and divergent) representations of your data. Any change that either you or your colleague make to the data is not mirrored. Which one is correct? What happens when you put a copy on the laboratory computer and do some analyses with it, then copy it to your ‘backup drive' (or even worse a thumb drive) and then put a copy of it on your laptop. Then when you get home, a copy goes on your home computer. How many different copies of your data are there in the world? Which one is correct? Somewhere down the road, you will need to recreate a figure for a manuscript, give someone a copy of your data, reanalyze your raw data with a new approach that you run across, etc. Which copy of the data that you haven't worked on for the last 6+ months is the right one? Which one is valid? Even if you use an awesome naming scheme like “Data.last.version.xls”, “Data.really.last.version.xls”, “I.totally.mean.last.version.xls” etc.. Not going to serve you well in the long run. All of this is terrifying from the perspective of reproducibility.

One way to keep data available with integrity is to have it located in one, and only one, location. If you use Google Drive, this is a pretty easy thing to do. You can keep your spreadsheet online, and then just access the data for your analysis when you need to.

## Keeping Data on Google Drive

Here is how you set that up. Upload your data into a Google Spreadsheet and then select File $\to$ Publish to the Web...  

You will be given a dialog (as shown here) that allows you to determine how much of the spreadsheet to share.  You can publish a particular sheet or the entire document.  For our purposes, it is probably best to keep our data on a single sheet within the file and publish them independently.  You can also determine the format you want the data presented in, at present the following are available:

* Web page - An html equivalent of the page  
* Comma-separated values (*.csv) - This is the preferred format for our purposes.  
* Tab-separated values (*.tsv) - Columns separated by tabs.  This is also an option for our needs.  
* PDF Document (*.pdf) - Portable Document Formats for general dissemination.  
* Microsoft Excel (*.xlsx) - Excel formats are great for excel but not openly available.
* OpenDocument Spreadsheet (*.ods) - Spreadsheet file format from OpenOffice.

```{r echo=FALSE}
knitr::include_graphics("./media/GoogleShare.png")
```

It is in my opinion preferable to keep your data in a format that is accessible to the broadest audience.  It is not a secret that programs change file formats and old proprietary binary formats can become corrupted and unreadable.  It would be a shame to keep the data that you work so hard on in a format that you can no longer access or that your potential future colleagues and collaborators cannot get access to...

For this example, the data from the first sheet are provided as a *.csv file and can be accessed by that long URL.

In R, we can access this spreadsheet directly through that URL, which allows us to have our data located in a single centralized location and accessible by a broad range of applications and uses, none of which have the ability to duplicate your data (thereby causing a fork) or make any changes to its internal structure.  Here is how it is done.
Load in the RCurl library that allows us to grab the contents at a site programatically.

```{r}
library(RCurl)
link <- "https://docs.google.com/spreadsheets/d/1QL9fYeKkDKphba12WLVTBJrv_d1WHTc9SrZoBeIFgj8/pub?gid=0&single=true&output=csv"
url <- getURL( link )
```

Initiate a text connection between R and the remote server at Google.

```{r}
con <- textConnection( url )
```

Load in the gstudio library and read the data in as a normal data file.  This last part is identical to what we would do for a normal file on our computer, it is the first two steps that extend this approach to use remote files via a url.

```{r message=FALSE}
library(gstudio)
cornus <- read_population( con, type="column", locus.columns = 5:14, sep=",")
summary(cornus)
```

The key here is that the contents of the data file are available but you cannot manipulate it. If you do need to make changes to the raw data, you can do this through the Google Drive interface.  Once you do that, all you would have to do is rerun your scripts and every one of them will point to the only and latest version of the data set.

I recommend looking into more approaches on Reproducible Research as they will help you, professionally, get the most out of your efforts.












